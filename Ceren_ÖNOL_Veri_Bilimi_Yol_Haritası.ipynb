{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzdw/uUMIVpnuYpJjQq47S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cerenonol/FordOtosan-GelecekHayalimProject-VeriBilimiYolHaritasi-AliBayram-CareerHubT/blob/main/Ceren_%C3%96NOL_Veri_Bilimi_Yol_Haritas%C4%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5kFlI1fY_8j_",
        "outputId": "ece16ada-066a-46c3-baa1-c8ecb81d3dcb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c671480a-016f-453a-ab89-5f0cce8b55fa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c671480a-016f-453a-ab89-5f0cce8b55fa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Ceren ÖNOL Veri Bilimi.docx to Ceren ÖNOL Veri Bilimi (2).docx\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Ceren ÖNOL\n",
            "\n",
            "Career HuBT – Veri Bilimi Eğitimi \n",
            "Eğitim İsmi: Veri Bilimi \n",
            "Eğitim Süresi: 60 Saat \n",
            " \n",
            "Eğitim İçeriği \n",
            "Makine Öğrenmesi \n",
            "Makine Öğrenmesi Temelleri \n",
            "Makine Öğrenmesi Nedir? \n",
            "Makine öğrenmesi, verilerden öğrenerek kararlar alabilen ve tahminlerde bulunabilen algoritmaların geliştirilmesini sağlayan bir yapay zeka dalıdır. Bu yöntem, sistemlerin deneyimle gelişmesini sağlar ve insanlar gibi öğrenme yetenekleri kazandırır.\n",
            "• Makine öğrenmesi vs Geleneksel Programlama \n",
            "Geleneksel programlamada, programcı kuralları ve mantığı belirleyerek sistemi kodlar; makine öğrenmesinde ise algoritmalar veri aracılığıyla bu kuralları kendileri keşfeder. Bu nedenle makine öğrenmesi, çok karmaşık ve kuralları önceden belirlenemeyen problemlerde daha etkilidir.\n",
            "• Hangi durumlarda kullanılabilir? \n",
            "Makine öğrenmesi, büyük miktarda veriden örüntüleri çıkarmak, tahminler yapmak ve karar almak gerektiğinde kullanılır. Özellikle tıp, finans, görüntü tanıma ve doğal dil işleme gibi alanlarda sıklıkla tercih edilir.\n",
            "\n",
            "2. Makine Öğrenmesi Çeşitleri \n",
            "• Supervised, Unsupervised, Reinforcement Learning \n",
            "Supervised (denetimli) öğrenmede, model eğitilirken doğru sonuçlar etiketlerle sağlanır ve model bu örneklerden öğrenir. Unsupervised (denetimsiz) öğrenmede etiketli veri kullanılmaz; model, veri içindeki gizli desenleri keşfetmeye çalışır, Reinforcement (pekiştirmeli) öğrenme ise ödül-ceza mekanizmasıyla çalışır ve model, doğru eylemleri keşfetmek için ortamla etkileşime girer.\n",
            "• Semi-supervised ve Self-supervised Learning \n",
            "Semi-supervised (yarı denetimli) öğrenme, az miktarda etiketli ve çok miktarda etiketlenmemiş veriyle çalışarak daha fazla bilgi çıkarımı sağlar. Self-supervised öğrenme ise veriden kendi etiketlerini oluşturarak kendini eğitir, bu yöntem özellikle veri etiketlemenin zor olduğu alanlarda kullanılır.\n",
            "\n",
            "3. Veri İşleme \n",
            "• Veri temizleme, özellik ölçeklendirme ve boyut azaltma teknikleri \n",
            "Veri temizleme, eksik veya yanlış verilerin ayıklanması ve düzeltilmesi sürecidir, modelin doğruluğunu artırır. Özellik ölçeklendirme, veriyi belli bir aralığa getirerek modelin daha hızlı ve doğru öğrenmesini sağlarken; boyut azaltma, gereksiz özellikleri kaldırarak modelin performansını iyileştirir.\n",
            "• Orantısız veri ile uğraşmak (imbalanced data set) \n",
            "Orantısız veri setlerinde bazı sınıflar diğerlerine göre daha az örneğe sahiptir, bu da modelin doğru tahmin yapmasını zorlaştırır. Bu sorunu çözmek için sınıf dengesini sağlamak üzere örneğin az olan sınıftan daha fazla veri eklemek veya özel algoritmalar kullanmak gerekir.\n",
            "\n",
            "4. Özellik Mühendisliği ve Seçimi \n",
            "• Özellik çıkarımı teknikleri \n",
            "Özellik çıkarımı, ham verilerden modelin öğrenmesine katkı sağlayacak yeni bilgiler veya değişkenler oluşturma sürecidir. Bu işlem, verinin anlamlı yapılar haline getirilmesini sağlayarak model performansını artırır.\n",
            "• Özellik seçim yöntemleri \n",
            "Özellik seçimi, modelin gereksiz veya az önemli özellikleri ele alarak daha etkili bir şekilde öğrenmesini sağlamak için en önemli değişkenleri seçme sürecidir. Bu yöntem, modelin hızını ve doğruluğunu artırırken karmaşıklığı azaltır.\n",
            "\n",
            "5. Model Değerlendirmesi ve Seçimi \n",
            "• Validasyon ve test seti seçmenin önemi \n",
            "Validasyon ve test setleri, modelin doğruluğunu ve genellenebilirliğini test etmek için veri setinden ayrılan özel parçalardır. Bu ayrım, modelin eğitim verisinde öğrendiklerini bağımsız veri üzerinde test ederek gerçek hayattaki performansını ölçmeyi sağlar.\n",
            "• sub-sample ile veri seçimini hızlandırmak \n",
            "Sub-sampling, büyük veri setlerinden daha küçük bir örnek seçerek modelin hızlı ve daha verimli bir şekilde eğitilmesini sağlar. Bu yöntem, model eğitimi için zaman ve kaynak tasarrufu sağlarken genel veri yapısını da yansıtmayı hedefler.\n",
            "• sub-sample seçim yöntemi \n",
            "Sub-sample seçiminde, ana veri kümesinden rastgele veya belirli kurallar çerçevesinde örnekler alınarak daha küçük bir veri seti oluşturulur. Bu yöntem, büyük veri setleri üzerinde çalışırken veri yükünü azaltmak için idealdir.\n",
            "• Cross-Validation \n",
            "Cross-validation, veriyi birçok parçaya bölerek her bir parçayı sırayla validasyon seti olarak kullanarak modelin performansını test etme yöntemidir. Bu teknik, modelin farklı veri bölümlerinde nasıl çalıştığını ölçerek daha güvenilir sonuçlar elde edilmesini sağlar.\n",
            "• Model karşılaştırma metrikleri \n",
            " Model karşılaştırma metrikleri, modellerin performansını kıyaslamak için kullanılan doğruluk, F1 skoru, hassasiyet gibi ölçütlerdir. Bu metrikler, hangi modelin daha iyi performans gösterdiğini anlamak için analiz yapılmasını sağlar.\n",
            "\n",
            "B. Denetimli Öğrenme Teknikleri \n",
            "Sınıflandırma \n",
            "\n",
            "6. Sınıflandırma Temelleri \n",
            "• Sınıflandırma nedir? \n",
            "Sınıflandırma, veriyi belirli kategorilere veya sınıflara ayırmak için kullanılan bir denetimli öğrenme tekniğidir. Bu yöntemle, model verilen girdiyi tanıyarak belirli sınıflardan birine tahmin etmeye çalışır, örneğin \"spam\" veya \"spam değil\" gibi.\n",
            "• Sınıflandırma Metrikleri (Accuracy, Precision, Recall, F1-Score) \n",
            "  Accuracy: Modelin doğru tahmin ettiği toplam tahmin oranını gösterir, ancak dengesiz veri setlerinde yanıltıcı olabilir.\n",
            "  Precision: Modelin belirli bir sınıf için yaptığı doğru pozitif tahminlerin tüm pozitif tahminlere oranıdır, yanlış alarmları azaltmada önemlidir.\n",
            "  Recall: Gerçek pozitiflerin ne kadarının doğru tahmin edildiğini gösterir, özellikle kaçırılmaması gereken sınıflarda önem taşır.\n",
            "  F1-Score: Precision ve recall'un harmonik ortalamasıdır, dengesiz veri setlerinde daha güvenilir bir performans ölçütü sunar.\n",
            "\n",
            "\n",
            "7. Linear Modeller \n",
            "• Logistic Regression \n",
            "Logistic regression, sınıflandırma problemlerinde kullanılan ve çıktıyı iki sınıf arasında bir olasılık değeri olarak tahmin eden bir lineer modeldir. Veriyi ayrıştırmak için sigmoid fonksiyonunu kullanır ve genellikle ikili sınıflandırma problemlerinde tercih edilir.\n",
            "• Support Vector Machines (SVM) \n",
            "Support Vector Machines, veriyi sınıflandırmak için en uygun ayrım çizgisini (veya hiperdüzlemi) bulmaya çalışan güçlü bir sınıflandırma algoritmasıdır. Özellikle karmaşık veri setlerinde ve sınıfların birbirine yakın olduğu durumlarda iyi performans gösterir.\n",
            "\n",
            "8. Ağaç Bazlı Modeller \n",
            "• Decision Trees \n",
            "Decision Trees, veriyi özelliklere göre dallara ayırarak kararlar veren ve her dalda bir sınıf tahmini yapan bir sınıflandırma modelidir. Kolay anlaşılabilir ve görselleştirilebilir olmakla birlikte, aşırı uyum (overfitting) yapma eğilimindedir.\n",
            "• Random Forests \n",
            "Random Forests, birden çok karar ağacından oluşan bir ansamble yöntemidir ve her ağaç bağımsız olarak karar verir, ardından sonuçlar birleştirilir. Bu yöntem, tek bir ağaç modeline göre daha güvenilir ve genellenebilir sonuçlar elde edilmesini sağlar.\n",
            "• Gradient Boosting Machines (e.g., XGBoost, LightGBM) \n",
            "Gradient Boosting Machines, zayıf öğrenicilerden (karar ağaçları gibi) oluşan bir ansamble modelidir ve her yeni ağaç, önceki ağaçların hatalarını düzeltmeye odaklanır. XGBoost ve LightGBM gibi versiyonları, modelin hızını ve doğruluğunu artıran optimizasyonlar ve düzenleme teknikleri sunar.\n",
            "\n",
            "9. Yapay Sinir Ağları \n",
            "• NN yapısı \n",
            "Yapay Sinir Ağı (Artificial Neural Network - ANN), insan beynindeki sinir hücrelerinin işleyişine benzer bir şekilde çalışan, bir giriş katmanından, bir veya daha fazla gizli katmandan ve bir çıkış katmanından oluşan bir modeldir. Her katmanda bulunan nöronlar, ağırlıklı bağlantılar aracılığıyla birbirine bağlanarak bilgiyi işler ve sonuca ulaşır.\n",
            " \n",
            "10. İleri Sınıflandırma Teknikleri \n",
            "• Ensemble Methods \n",
            "Ensemble Methods, birden fazla modelin bir arada kullanılarak daha güçlü ve doğru tahminler yapılmasını sağlayan tekniklerdir. Bu yöntemler, farklı modellerin hatalarını birbirini dengeleyerek daha genel ve güvenilir sonuçlar elde edilmesini sağlar (örneğin, Random Forest ve Gradient Boosting).\n",
            "• Imbalanced Datasets \n",
            "Imbalanced datasets, sınıfların dengesiz olduğu veri setleridir, yani bir sınıf diğerine göre çok daha fazla örneğe sahiptir. Bu durum, modelin azınlık sınıfı doğru tahmin etmede zorlanmasına yol açar, bu yüzden veri dengeleme teknikleri (örneğin, SMOTE) veya özel algoritmalar kullanılır.\n",
            "Regresyon \n",
            "Regresyon, sürekli bir hedef değişkeni tahmin etmek için kullanılan bir denetimli öğrenme tekniğidir. Girdi verileri ile hedef değişken arasındaki ilişkiyi modelleyerek, sayısal tahminler yapmayı amaçlar (örneğin, ev fiyatları tahmini).\n",
            "\n",
            "11. Regresyon Temelleri \n",
            "• Regresyon nedir? \n",
            "• Regresyon metrikleri (MSE, RMSE, MAE) \n",
            "  MSE (Mean Squared Error): Modelin tahmin ettiği değerler ile gerçek değerler arasındaki farkların karelerinin ortalamasını alır; yüksek MSE, modelin hata yaptığına işaret eder.\n",
            "\n",
            "  RMSE (Root Mean Squared Error): MSE'nin karekökünü alarak, modelin hata büyüklüğünü orijinal birimlerde ifade eder, bu da yorumlamayı kolaylaştırır.\n",
            "\n",
            "  MAE (Mean Absolute Error): Gerçek değerler ile tahmin edilen değerler arasındaki mutlak farkların ortalamasını alır; modelin hata büyüklüğünü daha basit bir şekilde ölçer.\n",
            "\n",
            "12. Lineer Regression \n",
            "• Simple ve Multiple Linear Regression \n",
            "  Simple Linear Regression: Bu model, tek bir bağımsız değişkenin hedef değişkenle olan doğrusal ilişkisinin incelendiği regresyon türüdür. Bir doğrunun denklemiyle model kurularak, bir veri setindeki iki değişken arasındaki ilişkiyi tahmin eder.\n",
            "\n",
            "  Multiple Linear Regression: Bu model, birden fazla bağımsız değişkenin hedef değişkenle olan doğrusal ilişkisini inceleyen regresyon türüdür. Birden çok özellik kullanılarak, daha karmaşık ilişkilerin modellemesi yapılır ve tahminler daha doğru hale gelir.\n",
            "• Regularization teknikleri (Ridge, Lasso) \n",
            "  Ridge Regression: Ridge, modelin karmaşıklığını kontrol etmek için L2 ceza terimi ekler, böylece modelin aşırı uyum yapmasının önüne geçer. Bu teknik, tüm özelliklerin katsayılarını küçültür, ancak sıfır yapmaz.\n",
            "\n",
            "  Lasso Regression: Lasso, L1 ceza terimi ekler ve bazı özelliklerin katsayılarını sıfırlayarak onları modelden çıkarır. Bu yöntem, daha az özellikle daha yalın ve anlamlı modeller oluşturulmasını sağlar.\n",
            "\n",
            "13. Non-Linear Regression \n",
            "• Polynomial Regression \n",
            "Polynomial Regression, verinin doğrusal olmayan ilişkilerini modellemek için kullanılan bir regresyon türüdür. Bu model, bağımsız değişkenleri polinom terimleriyle genişleterek daha karmaşık eğriler oluşturur ve doğrusal olmayan ilişkiyi daha iyi yakalar.\n",
            "• Decision Trees ve Random Forest \n",
            "  Decision Trees: Karar ağaçları, veriyi özelliklere göre dallara ayırarak, her dalda bir tahmin yapar ve doğrusal olmayan ilişkileri modellemek için etkilidir. Ağaç yapıları, özellikle karmaşık verilerde iyi performans gösterir.\n",
            "\n",
            "  Random Forest: Random Forest, birden çok karar ağacının birleşimidir ve her ağacın bağımsız olarak yaptığı tahminleri birleştirir. Bu yöntem, modelin doğruluğunu artırarak daha güvenilir ve genel sonuçlar elde edilmesini sağlar.\n",
            "• Gradient Boosting Machines (e.g., XGBoost, LightGBM) \n",
            "Gradient Boosting Machines, zayıf öğrenicilerden (karar ağaçları gibi) oluşan bir ansamble modelidir. Bu tür modellerde, her yeni model önceki modelin hatalarını düzeltmeye odaklanır; XGBoost ve LightGBM gibi popüler teknikler, bu süreci hızlandırarak yüksek doğruluk ve verimlilik sağlar.\n",
            "\n",
            "14. Support Vector Regression (SVR) \n",
            "Support Vector Regression, doğrusal olmayan veri setleri üzerinde regresyon yapmak için kullanılan bir yöntemdir. SVR, veriyi daha yüksek boyutlu bir uzaya haritalayarak, doğrusal olmayan ilişkileri modellemeye olanak tanır ve en iyi genelleme gücünü sağlamak için veriyi destek vektörleri etrafında optimize eder.\n",
            "• Kernel Tricks \n",
            "Kernel tricks, doğrusal olmayan ilişkileri modellemek için kullanılan bir tekniktir ve veriyi yüksek boyutlu bir uzaya dönüştürürken hesaplama maliyetlerini azaltır. Bu yöntem, çeşitli kernel fonksiyonları (örneğin, polinom, RBF) kullanarak veriyi daha karmaşık bir biçimde dönüştürür, böylece SVR'nin doğrusal olmayan veri üzerinde çalışmasını sağlar.\n",
            "\n",
            "15. Regresyon İçin Yapay Sinir Ağları \n",
            "Yapay Sinir Ağları, regresyon problemlerini çözmek için de kullanılabilir. Özellikle, karmaşık doğrusal olmayan ilişkileri modelleme yeteneği sayesinde, çok katmanlı sinir ağları (MLP - Multilayer Perceptrons) hedef değişkeni tahmin etmek için güçlü bir yöntem sunar.\n",
            "C. Denetimsiz Öğrenme Teknikleri \n",
            "Modeli kullanarak veri ile alakalı ileri düzey analiz yapmak \n",
            "Denetimsiz öğrenme, etiketlenmemiş veri ile modelleme yapmayı sağlar. Bu teknikler, veri setindeki desenleri, yapıları ve gizli ilişkileri keşfetmek için kullanılır; örneğin, kümeleme (Clustering), boyut indirgeme (Dimensionality Reduction) gibi yöntemlerle veriden anlamlı içgörüler çıkarılabilir. Bu analizler, veriyi daha iyi anlamaya ve gelecekteki modeller için daha sağlam özellikler elde etmeye yardımcı olur.\n",
            "\n",
            "16. Kümeleme Algoritmaları \n",
            "• K-Means, Hierarchical Clustering, DBSCAN \n",
            "  K-Means: K-Means, veriyi belirli sayıda kümeye ayırmak için kullanılan bir algoritmadır. Veriler, en yakın küme merkezine (centroid) atanarak, her küme için ortalama hesaplanır ve iteratif bir şekilde optimize edilir.\n",
            "\n",
            "  Hierarchical Clustering: Bu yöntem, veriyi hiyerarşik bir yapıda kümelemeye olanak tanır. Aglo-meratif (alt kümeler birleştirilir) ve Divizif (kümelemeler ayrılır) olmak üzere iki ana yaklaşım vardır.\n",
            "\n",
            "  DBSCAN (Density-Based Spatial Clustering of Applications with Noise): DBSCAN, verinin yoğunluk bölgelerini bulmaya çalışan bir algoritmadır ve gürültüyü (outlier) ayırarak kümeler oluşturur. Bu yöntem, özellikle farklı yoğunluklardaki kümeleri algılamada etkilidir.\n",
            "• Kümeleme performansının ölçümü \n",
            "Kümeleme algoritmalarının performansı, genellikle dışsal (veri etiketlerine dayalı) veya içsel (veri içindeki yapıyı analiz ederek) ölçümlerle değerlendirilir. Dışsal ölçütler arasında Silhouette Score ve Adjusted Rand Index (ARI) yer alırken, içsel ölçütler arasında Davies-Bouldin Index ve Dunn Index kullanılır. Bu metrikler, kümeleme sonuçlarının doğruluğunu ve anlamlılığını belirlemede yardımcı olur.\n",
            "\n",
            "17. Boyut Azaltımı ve Manifold Learning \n",
            "\n",
            "\n",
            "• Principal Component Analysis (PCA) \n",
            "Principal Component Analysis (PCA), verinin boyutunu azaltmak için kullanılan bir tekniktir. PCA, verideki en büyük varyansı açıklayan doğrusal bileşenleri (principal components) bulur ve verinin boyutunu bu bileşenler üzerinde projeksiyon yaparak azaltır, böylece daha düşük boyutlu bir veri seti elde edilir.\n",
            "• t-SNE, UMAP for Non-linear Dimensionality Reduction \n",
            "  t-SNE (t-distributed Stochastic Neighbor Embedding), veriyi iki veya üç boyutlu uzaya indirgemek için kullanılan bir tekniktir ve özellikle verinin non-lineer yapısını koruyarak kümeleri görselleştirmeye yardımcı olur. t-SNE, özellikle yüksek boyutlu verilerin görsel analizi için uygundur.\n",
            "\n",
            "  UMAP (Uniform Manifold Approximation and Projection), t-SNE'ye benzer bir boyut indirgeme yöntemidir, ancak daha hızlıdır ve büyük veri setlerinde daha etkili çalışır. UMAP, verinin manifold yapısını koruyarak daha anlamlı ve ayrık düşük boyutlu temsiller oluşturur.\n",
            "\n",
            "18. Association Rule Mining \n",
            "• Uygulamalar \n",
            "Association Rule Mining, veri madenciliği alanında, veriler arasındaki ilişkileri keşfetmek için kullanılan bir tekniktir. Bu yöntem, özellikle perakende sektöründe, müşterilerin birlikte satın aldığı ürünleri belirleyerek, çapraz satış stratejileri geliştirmeye yardımcı olur. Örneğin, \"Ekmek alan müşteriler genellikle tereyağı da alır\" gibi kurallar, pazarlama kampanyaları ve envanter yönetimi için kullanılabilir. Ayrıca, sağlık, bankacılık ve sosyal medya gibi farklı alanlarda da ilişkisel desenleri tespit etmek için kullanılır.\n",
            "\n",
            "19. Feature Importance \n",
            "Feature importance, bir modelin tahminlerinde hangi özelliklerin (özellikler) daha fazla etkili olduğunu belirlemek için kullanılan bir tekniktir. Bu metrik, her özelliğin modelin kararlarına katkısını ölçer ve daha anlamlı, etkili özelliklere odaklanılmasına yardımcı olur.\n",
            "\n",
            "20. Permutation Feature Importance \n",
            "Permutation feature importance, modelin tahminlerinin doğruluğunu değerlendirirken her bir özelliğin önemini belirler. Bu yöntem, bir özelliği rastgele karıştırarak modelin doğruluğundaki değişikliği ölçer; daha büyük bir doğruluk kaybı, o özelliğin daha önemli olduğunu gösterir.\n",
            "\n",
            "21. Partial Dependence \n",
            "Partial dependence, bir veya daha fazla özellik ile hedef değişken arasındaki ilişkiyi görselleştiren bir tekniktir. Bu yöntem, diğer tüm özellikler sabit tutularak, sadece bir özelliğin değeri değiştirildiğinde modelin tahmininin nasıl değiştiğini gösterir, bu da bir özelliğin hedef üzerinde nasıl bir etkisi olduğunu anlamaya yardımcı olur.\n",
            " \n",
            "D. Derin Öğrenme Temelleri \n",
            "1. Derin Öğrenmeye Giriş \n",
            "1. Derin Öğrenme Nedir? \n",
            "Derin öğrenme, yapay sinir ağlarını kullanarak verilerden öğrenmeyi sağlayan bir makine öğrenme alt alanıdır. Bu yöntem, özellikle büyük veri setleriyle çalışırken, modelin katmanlar aracılığıyla daha soyut ve derin temsiller öğrenmesine olanak tanır.\n",
            "• Makine öğrenmesi vs. derin öğrenme \n",
            "Makine öğrenmesi, verilerden belirli kalıpları öğrenerek tahminlerde bulunan bir tekniktir; genellikle daha az veriyle iyi sonuçlar verebilir. Derin öğrenme ise, çok katmanlı yapay sinir ağları kullanarak karmaşık veri yapılarından anlam çıkarır ve genellikle büyük veri setleri ve güçlü hesaplama gereksinimleri ile daha iyi sonuçlar elde edilir.\n",
            "• Derin öğrenmenin tarihçesi ve gelişimi \n",
            "Derin öğrenme, ilk olarak 1950'lerde yapay sinir ağlarının geliştirilmesiyle başladı, ancak hesaplama gücünün artması ve büyük veri setlerinin bulunmasıyla 2010'larda büyük bir ivme kazandı. Derin öğrenme, özellikle görüntü işleme, doğal dil işleme ve ses tanıma gibi alanlarda devrim yaratmıştır.\n",
            "\n",
            "2. Yapay Sinir Ağları Temelleri \n",
            "• Aktivasyon ve türevlenebilir hesaplamalar kavramı \n",
            "Aktivasyon fonksiyonu, bir nöronun çıktısını belirlemek için kullanılan matematiksel bir fonksiyondur. Bu fonksiyon, ağın doğrusal olmayan ilişkileri öğrenmesini sağlar ve türevlenebilirliği, ağın geri yayılım (backpropagation) algoritması ile öğrenmeyi mümkün kılar, çünkü ağın hatalarını geriye doğru iletebilmek için türev hesaplanması gereklidir.\n",
            "• Nöron yapısı: girdi, ağırlıklar, bias, aktivasyon fonksiyonu \n",
            "Bir nöron, bir giriş (input), girişin ağırlıkları (weights), ve bir bias terimi ile çalışır. Girdi, modelin aldığı veriyi, ağırlıklar bu verinin model için ne kadar önemli olduğunu belirler, bias ise çıktının yerini ayarlayarak modelin doğruluğunu artırır. Aktivasyon fonksiyonu ise bu hesaplanan değerlerin, ağın çıktısını belirlemek için geçirdiği fonksiyondur (örneğin ReLU, Sigmoid, Tanh).\n",
            "\n",
            "3. Feed Forward Neural Network Yapısı \n",
            "• Single layer perceptron \n",
            "Single Layer Perceptron (SLP), bir tek gizli katman ve bir çıkış katmanından oluşan basit bir yapay sinir ağıdır. Bu ağ, doğrusal karar sınırları oluşturur ve yalnızca doğrusal sınıflandırma problemlerini çözebilir; her bir girişin çıktıya olan etkisini ağırlıklarla belirler.\n",
            "• Multi layer perceptron (MLP) \n",
            "Multi Layer Perceptron (MLP), birden fazla gizli katman içeren ve her katmanda birden çok nöron bulunan bir yapıdır. MLP, doğrusal olmayan sınıflandırma ve regresyon problemlerini çözebilme yeteneğine sahiptir, çünkü her katman, daha karmaşık ve soyut özellikleri öğrenerek daha güçlü ve esnek modeller oluşturur. Bu yapı, ardışık katmanlar arasında aktivasyon fonksiyonları kullanarak veriyi işler ve öğrenir.\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "2. Derin Öğrenme Bileşenleri \n",
            "1. Aktivasyon Fonksiyonları \n",
            "• Sigmoid, tanh, ReLU ve türevleri \n",
            "  Sigmoid: Sigmoid fonksiyonu, giriş değerini 0 ile 1 arasında bir değere dönüştüren S şeklinde bir fonksiyondur. Çıkışı, özellikle sınıflandırma problemlerinde olasılık tahminleri için uygundur, ancak gradyan kaybı sorununa (vanishing gradient) yol açabilir.\n",
            "\n",
            "  Tanh: Tanh (hiperbolik tanjant) fonksiyonu, giriş değerini -1 ile 1 arasında dönüştürür. Sigmoid’e benzer, ancak sıfır etrafında daha güçlü gradyanlar sağlar, bu da öğrenmeyi daha hızlı hale getirebilir, fakat yine de vanishing gradient problemiyle karşılaşabilir.\n",
            "  ReLU: ReLU (Rectified Linear Unit), negatif değerleri sıfıra indirirken pozitif değerleri olduğu gibi geçiren doğrusal olmayan bir aktivasyon fonksiyonudur. ReLU, hızlı öğrenme ve derin ağlarda iyi performans gösterir, ancak \"ölü nöronlar\" (dead neurons) sorununa yol açabilir.\n",
            "  ReLU Türevleri: ReLU’nun türevi, giriş pozitifse 1, negatifse 0’dır. Bu basit türev hesaplama, geri yayılım sırasında hızlı ve verimli gradyan hesaplamalarına olanak sağlar.\n",
            "• Aktivasyon fonksiyonlarının karşılaştırılması \n",
            "  Sigmoid ve Tanh: Sigmoid, özellikle sınıflandırma problemlerinde kullanışlıdır, ancak sınırlı değer aralığı ve gradyan kaybı sorunları yaşanabilir. Tanh, sıfır etrafında daha güçlü gradyanlar sağlar, ancak yine de aynı vanishing gradient problemini barındırır.\n",
            "  ReLU: ReLU, büyük derinlikli ağlarda daha hızlı öğrenmeye olanak tanır ve genellikle daha iyi performans gösterir. Ancak, negatif girişlerde gradyanların sıfırlanması nedeniyle ölü nöron problemi ortaya çıkabilir. Bununla birlikte, Leaky ReLU gibi türevleri bu sorunu kısmen çözer.\n",
            "2. Loss Fonksiyonları\n",
            "Mean Squared Error (MSE): Modelin tahminlerinin gerçek değerlere olan farklarının karesinin ortalamasını alır, genellikle regresyon problemlerinde kullanılır.\n",
            "Cross-Entropy Loss: Sınıflandırma problemlerinde kullanılır ve modelin doğru sınıfı tahmin etme olasılığı ile gerçek etiket arasındaki farkı ölçer.\n",
            "Hinge Loss: SVM gibi modellerde kullanılan, doğru sınıfın diğer sınıflardan daha fazla uzak olmasını hedefleyen bir kayıp fonksiyonudur.\n",
            "3. Optimizasyon Algoritmaları\n",
            "Gradient Descent ve türevleri (SGD, Mini-batch GD): Modelin parametrelerini, kayıp fonksiyonunun gradyanı doğrultusunda adım adım optimize eder. Stokastik gradient descent (SGD) ve mini-batch gradient descent gibi türevleri kullanılır.\n",
            "Momentum, RMSprop, Adam: Bu algoritmalar, gradient descent algoritmasının hızını ve doğruluğunu iyileştirir, özellikle daha büyük veri setleri ve karmaşık modeller için tercih edilir.\n",
            "4. Backpropagation Algoritması\n",
            "Zincir kuralı ve gradyan hesaplama: Derin öğrenme modelinin her katmanındaki parametrelerin güncellenmesi için zincir kuralı kullanılarak gradyan hesaplanır.\n",
            "Backpropagation adım adım uygulanması: Modelin çıktı hatasından başlayarak her katmanda bu hata geriye doğru iletilir ve parametreler güncellenir.\n",
            "3. Model Eğitimi ve Değerlendirme\n",
            "1. Veri Hazırlama\n",
            "Veri ön işleme teknikleri: Veriyi temizlemek, eksik verileri işlemek ve özellik mühendisliği yapmak, modelin başarısını artırmak için gereklidir.\n",
            "Normalizasyon ve standardizasyon: Verinin belirli bir aralıkta ya da ortalama sıfır, standart sapma bir olacak şekilde düzenlenmesi, modelin daha hızlı ve verimli çalışmasına yardımcı olur.\n",
            "2. Eğitim Süreci\n",
            "Epoch vs Batch kavramları: Epoch, tüm veri setinin model üzerinden bir kez geçmesi, batch ise verinin küçük parçalara bölünüp işlenmesidir.\n",
            "Batch size seçimi ve etkileri: Batch boyutu, modelin eğitim süresi ve genel başarısını etkileyen önemli bir parametredir.\n",
            "3. Overfitting ve Underfitting\n",
            "Bias-variance trade-off: Modelin yeterince esnek olmaması (underfitting) ya da aşırı esnek olup veri gürültüsünü öğrenmesi (overfitting) arasında denge kurulmalıdır.\n",
            "Regularizasyon teknikleri (L1, L2): Aşırı öğrenmeyi engellemek için model parametrelerine ceza ekleyen tekniklerdir.\n",
            "4. Model Değerlendirme\n",
            "Train/Validation/Test set ayrımı: Modelin genel performansını değerlendirmek için veri seti, eğitim, doğrulama ve test olmak üzere ayrılır.\n",
            "Cross-validation teknikleri: Modelin daha güvenilir bir şekilde değerlendirilmesi için k-katlamalı çapraz doğrulama kullanılır.\n",
            "4. İleri Derin Öğrenme Teknikleri\n",
            "1. Batch Normalization\n",
            "Internal covariate shift problemi: Eğitim sırasında ağırlıkların ve aktivasyonların dağılımındaki değişim, öğrenmeyi zorlaştırır. Batch normalization bu sorunu çözer.\n",
            "Batch norm'un çalışma prensibi ve avantajları: Aktivasyonların dağılımını sabitleyerek daha hızlı ve daha stabil bir eğitim sağlar.\n",
            "2. Weight Initialization\n",
            "Xavier/Glorot initialization: Ağın ilk ağırlıklarını uygun bir şekilde başlatmak, ağın daha verimli öğrenmesini sağlar.\n",
            "He initialization: Derin ağlarda ReLU aktivasyon fonksiyonu kullanıldığında, ağırlıkların daha uygun şekilde başlatılması için tercih edilen bir tekniktir.\n",
            "3. Dropout\n",
            "Overfitting'i önlemede dropout'un rolü: Eğitim sırasında bazı nöronları rastgele devre dışı bırakarak aşırı öğrenmeyi engeller.\n",
            "Dropout uygulama stratejileri: Genellikle %20-%50 oranında nöron bırakılır.\n",
            "4. Transfer Learning\n",
            "Pre-trained modeller ve fine-tuning: Daha önce eğitilmiş modellerin, yeni bir görev için yeniden eğitilmesi, transfer learning olarak bilinir.\n",
            "Transfer learning'in avantajları ve uygulama alanları: Küçük veri setlerinde daha iyi sonuçlar elde etmek için kullanılır.\n",
            "5. Derin Öğrenme Mimarileri\n",
            "1. Evrişimli Sinir Ağları (CNN)\n",
            "Evrişim işlemi ve özellik haritaları: CNN'ler, görüntülerdeki önemli özellikleri öğrenmek için evrişim (convolution) işlemi kullanır.\n",
            "Pooling katmanları: Özelliklerin boyutunu küçültüp, modelin hesaplama yükünü azaltan işlemlerdir.\n",
            "Klasik CNN mimarileri (LeNet, AlexNet, VGG): Bu mimariler, evrişimli sinir ağlarının temellerini atmıştır.\n",
            "2. Sequence Modeling\n",
            "Tekrarlayan Sinir Ağları (RNN): Özellikle zaman serisi verileri ve doğal dil işleme için uygun bir ağ yapısıdır.\n",
            "LSTM ve GRU birimleri: RNN'lerin sınırlamalarını aşmak için kullanılan, daha uzun süreli bağıntıları öğrenebilen gelişmiş yapılandırmalardır.\n",
            "Bidirectional RNNs: Zaman sırasıyla veriyi her iki yönden de işleyen bir ağ yapısıdır.\n",
            "3. Autoencoder'lar\n",
            "Encoder-decoder yapısı: Autoencoder, veriyi sıkıştırıp daha sonra tekrar orijinal haline getirmeyi amaçlar.\n",
            "Denoising autoencoder'lar: Gürültülü veriyi temizlemek için kullanılan bir tür autoencoder modelidir.\n",
            "4. Generative Adversarial Networks (GAN)\n",
            "Generator ve discriminator kavramları: GAN'lerde bir ağ, yeni veri örnekleri üretirken, diğeri bu örneklerin gerçek olup olmadığını kontrol eder.\n",
            "GAN eğitim süreci ve zorlukları: Eğitim sırasında generator ve discriminator birbirine karşı yarışarak gelişir.\n",
            "6. Derin Öğrenme Optimizasyonu\n",
            "1. Hiperparametre Optimizasyonu\n",
            "Grid search ve random search: Hiperparametrelerin en uygun değerlerini bulmak için kullanılan yöntemlerdir.\n",
            "Bayesian optimizasyon: Hiperparametre arama alanında daha verimli bir yaklaşım sunar.\n",
            "2. Model Compression\n",
            "Pruning teknikleri: Ağaç yapılarındaki gereksiz dalların kesilmesiyle modelin boyutunun küçültülmesidir.\n",
            "Quantization: Modelin parametrelerinin daha düşük bitlerle temsil edilmesini sağlar.\n",
            "3. Gradient Clipping\n",
            "Exploding gradient problemi: Modelin eğitiminde, gradyanların çok büyük olması sorununu çözmek için kullanılan bir tekniktir.\n",
            "Clipping teknikleri ve uygulamaları: Gradyanları bir eşik değeriyle sınırlayarak modelin stabilitesini artırır.\n",
            "4. Learning Rate Scheduling\n",
            "Step decay, exponential decay: Öğrenme oranının eğitim süreci boyunca azalması, daha iyi genelleme yapmayı sağlar.\n",
            "Cyclical learning rates: Öğrenme oranının belirli aralıklarla artıp azaldığı bir stratejidir.\n",
            "7. Derin Öğrenme Frameworks\n",
            "1. PyTorch\n",
            "PyTorch tensors ve autograd: Tensors, veriyi işlemek için temel yapı taşlarıdır; autograd ise otomatik türev hesaplamasını sağlar.\n",
            "PyTorch ile model tanımlama ve eğitim: PyTorch, dinamik hesaplama grafiği kullanarak model geliştirme ve eğitim sürecini kolaylaştırır.\n",
            "2. TensorFlow ve Keras\n",
            "TensorFlow temelleri: Derin öğrenme modellerini oluşturmak için geniş bir ekosisteme sahip bir framework'tür.\n",
            "Keras API ile model oluşturma: Keras, TensorFlow'un üst katman bir API'sidir ve hızlı prototip oluşturmayı sağlar.\n",
            "3. Framework Karşılaştırması\n",
            "TensorFlow vs PyTorch: TensorFlow daha çok üretim ortamlarında kullanılırken, PyTorch araştırma ve geliştirme için daha uygundur.\n",
            "Endüstri ve araştırma kullanım senaryoları: PyTorch genellikle akademik çalışmalar için tercih edilirken, TensorFlow endüstri çözümleri için daha yaygın olarak kullanılmaktadır.\n",
            "8. Derin Öğrenmenin Uygulama Alanları\n",
            "1. Bilgisayarlı Görü\n",
            "Görüntü sınıflandırma ve nesne tespiti: Görüntülerdeki nesneleri tanıma ve etiketleme işlemleridir.\n",
            "Semantic segmentation: Görüntülerdeki her pikselin anlamlı bir etikete atanmasıdır.\n",
            "2. Doğal Dil İşleme\n",
            "Metin sınıflandırma ve duygu analizi: Metinlerin anlamını çıkarmak ve duygusal içeriklerini analiz etmek.\n",
            "Makine çevirisi ve metin üretimi: Bir dildeki metni başka bir dile çevirmek ya da metin üretmek için kullanılan modellerdir.\n",
            "3. Oyun ve Simülasyon\n",
            "Reinforcement learning (RL) ve Deep Q-Networks (DQN): Yapay zeka ajanlarının çevreleriyle etkileşime girerek en iyi eylem stratejisini öğrenmesini sağlar.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "!pip install python-docx\n",
        "\n",
        "from docx import Document\n",
        "\n",
        "\n",
        "doc = Document('Ceren ÖNOL Veri Bilimi.docx')\n",
        "\n",
        "\n",
        "for para in doc.paragraphs:\n",
        "    print(para.text)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A58Yc8eV__f2"
      }
    }
  ]
}